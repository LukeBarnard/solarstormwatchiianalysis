{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import SolarStormwatchIIAnalysis as ssw\n",
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, invalid path, check config: E:\\STEREO\\ares.nrl.navy.mil\\lz\n",
      "Error, invalid path, check config: E:\\STEREO\\ares.nrl.navy.mil\\lz\n"
     ]
    }
   ],
   "source": [
    "classifications = ssw.import_classifications(latest=True)\n",
    "\n",
    "subjects = ssw.import_subjects(active=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, invalid path, check config: E:\\STEREO\\ares.nrl.navy.mil\\lz\n",
      "Error, invalid path, check config: E:\\STEREO\\ares.nrl.navy.mil\\lz\n",
      "{'img_type': 'norm', 'subject_id': '0', 'subject_name': 'ssw_009_swpc_638_sta_norm_20121005T092901_20121005T104901', 'asset_2': 'ssw_009_swpc_638_sta_norm_20121005_104901.jpg', 'asset_0': 'ssw_009_swpc_638_sta_norm_20121005_092901.jpg', 'asset_1': 'ssw_009_swpc_638_sta_norm_20121005_100901.jpg'}\n",
      "T1\n"
     ]
    }
   ],
   "source": [
    "reload(ssw)\n",
    "\n",
    "classifications = ssw.import_classifications(latest=True)\n",
    "\n",
    "subjects = ssw.import_subjects(active=True)\n",
    "\n",
    "for ids, sub in subjects.iterrows():\n",
    "    print sub['metadata']\n",
    "    break\n",
    "\n",
    "for ids, cls in classifications.iterrows():\n",
    "    print cls['annotations']['task']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ssw_009': {'sta': {'diff': [], 'norm': []}, 'stb': {'diff': [], 'norm': []}}}\n",
      "{'ssw_009': {'sta': {'diff': [3215323L, 3215327L, 3215332L, 3215339L, 3215347L, 3215354L, 3215360L, 3215366L, 3215372L], 'norm': [3215239L, 3215249L, 3215259L, 3215267L, 3215276L, 3215287L, 3215296L, 3215305L, 3215314L]}, 'stb': {'diff': [3215672L, 3215679L, 3215686L, 3215693L, 3215699L, 3215704L, 3215711L, 3215718L, 3215726L], 'norm': [3215595L, 3215606L, 3215617L, 3215626L, 3215636L, 3215644L, 3215645L, 3215650L, 3215661L]}}}\n"
     ]
    }
   ],
   "source": [
    "subjects = ssw.import_subjects(active=True)\n",
    "\n",
    "event_ids = []\n",
    "\n",
    "for ids, sub in subjects.iterrows():\n",
    "    meta = json.loads(sub['metadata'])\n",
    "    name = meta['subject_name']\n",
    "    ssw_code = name.split('_')\n",
    "    ssw_code = \"_\".join(ssw_code[0:2])\n",
    "    event_ids.append(ssw_code)\n",
    "\n",
    "# Get unique event id's\n",
    "event_ids = list(set(event_ids))\n",
    "\n",
    "event_tree = dict()\n",
    "for e in event_ids:\n",
    "            event_tree[e] = {'sta':{'norm':[], 'diff':[]}, 'stb':{'norm':[], 'diff':[]}}\n",
    "\n",
    "# Now populate the lists in the subject sets with the individual subsets \n",
    "for ids, sub in subjects.iterrows():\n",
    "    meta = json.loads(sub['metadata'])\n",
    "    name = meta['subject_name']\n",
    "    name = name.split('_')\n",
    "    ssw_code = \"_\".join(name[0:2])\n",
    "    craft  = name[4]\n",
    "    im_type = name[5]\n",
    "    \n",
    "    event_tree[ssw_code][craft][im_type].append(sub['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a dictionary with details of which subjects and subject sets belong to which \n",
    "# Get subject sets belonging to each event\n",
    "event_dict = dict()\n",
    "for e in event_ids:\n",
    "    subject_sets = []\n",
    "    for ids, sub in subjects.iterrows():\n",
    "        meta = json.loads(sub['metadata'])\n",
    "        name = meta['subject_name']\n",
    "        ssw_code = name.split('_')\n",
    "        ssw_code = \"_\".join(ssw_code[0:2])\n",
    "        if ssw_code == e:\n",
    "            subject_sets.append(sub['subject_set_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop through the unique subject sets\n",
    "    for set_id in subjects['subject_set_id'].unique():\n",
    "        # Get all subjects in this set\n",
    "        subject_subset = get_df_subset(subjects, 'subject_set_id', set_id)\n",
    "\n",
    "        # Get info on the files in this set.\n",
    "        set_files = get_subject_set_files(subject_subset, [set_id])\n",
    "\n",
    "        # Create a group for this set\n",
    "        grp_set = hdf.create_group(str(set_id))\n",
    "        grp_set.attrs['craft'] = set_files[set_id]['craft']\n",
    "\n",
    "        # Loop over the subjects in this set\n",
    "        for ids, sub in subject_subset.iterrows():\n",
    "            # Get the meta information of assets in the subject\n",
    "            info = json.loads(sub['metadata'])\n",
    "\n",
    "            # Find classifications for this subject\n",
    "            clas_subset = get_df_subset(classifications, 'subject_id', sub['subject_id'])\n",
    "\n",
    "            # Loop over the assets in this subject\n",
    "            for asset_id in range(3):\n",
    "                key = \"asset_{}\".format(asset_id)\n",
    "                # Form a timestring for the hdf5 branch name\n",
    "                file_parts = os.path.splitext(info[key])[0].split('_')\n",
    "                timestr = \"_\".join(file_parts[6:8])\n",
    "                timestr = pd.datetime.strptime(timestr, '%Y%m%d_%H%M%S').isoformat()\n",
    "\n",
    "                # Make a branch for this image type / asset time.\n",
    "                branch = \"/\".join([info['img_type'], timestr])\n",
    "                print branch\n",
    "                grp_asset = grp_set.create_group(branch)\n",
    "\n",
    "                # Loop over the classifications to pull out the annotations for this asset.\n",
    "                all_x_pix = []\n",
    "                all_y_pix = []\n",
    "                all_user = []\n",
    "                # Counter for the number of submissions on this asset\n",
    "                submission_count = 0\n",
    "                for idc, cla in clas_subset.iterrows():\n",
    "\n",
    "                    # Get the annotation\n",
    "                    cla_anno = json.loads(cla['annotations'])[0]\n",
    "\n",
    "                    # Loop through values and find value with frame matching asset index\n",
    "                    for val in cla_anno['value']:\n",
    "                        # Does this frame match the asset of interest?\n",
    "                        if val['frame'] == asset_id:\n",
    "                            # Get the data for this user classification\n",
    "                            user_x_pix = [int(p['x']) for p in val['points']]\n",
    "                            user_y_pix = [int(p['y']) for p in val['points']]\n",
    "                            user_id = cla['user_id']\n",
    "                            # Set up a group for this user.\n",
    "                            branch = \"users/user_{}\".format(submission_count)\n",
    "                            print \"---->\" + branch\n",
    "                            grp_user = grp_asset.create_group(branch)\n",
    "                            # Assign the user data to their group\n",
    "                            grp_user.create_dataset('x_pix', data=np.array(user_x_pix, dtype=int))\n",
    "                            grp_user.create_dataset('y_pix', data=np.array(user_y_pix, dtype=int))\n",
    "                            grp_user.create_dataset('id', data=user_id)\n",
    "                            # Dump users data into collection of annotations.\n",
    "                            all_x_pix.extend(user_x_pix)\n",
    "                            all_y_pix.extend(user_y_pix)\n",
    "                            all_user.append(user_id)\n",
    "                            submission_count += 1\n",
    "\n",
    "                # Now add on the collected annotation to the assets group.\n",
    "                grp_asset.create_dataset('x_pix', data=np.array(all_x_pix, dtype=int))\n",
    "                grp_asset.create_dataset('y_pix', data=np.array(all_y_pix, dtype=int))\n",
    "                grp_asset.create_dataset('user_list', data=np.array(all_user, dtype=int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
